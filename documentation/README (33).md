# Intelligent Folder Organizer with RAG-Ready Indexing

## ğŸš€ Overview

This is a **complete upgrade** from basic file organization to an intelligent, agent-ready knowledge management system. Unlike simple file movers, this system:

1. **Understands content** - Analyzes file contents, not just extensions
2. **Creates searchable knowledge** - Generates embeddings and semantic search indexes
3. **Produces agent-ready formats** - JSONL chunks, vector databases, and API endpoints
4. **Works with any folder** - Self-adaptive categorization for any domain
5. **Preserves context** - Maintains semantic relationships and document structure

## ğŸ“Š Comparison: Before vs After

### Original Approach
```python
# Simple file mover based on extensions
PDF â†’ Documents/PDFs
*.py â†’ Code/Scripts
```
- âŒ No content analysis
- âŒ No search capability
- âŒ No metadata extraction
- âŒ Not queryable by agents
- âŒ Hardcoded categories

### New Intelligent System
```python
# Content-aware knowledge management
PDF â†’ Analyzed â†’ Chunked â†’ Embedded â†’ Indexed â†’ Searchable
```
- âœ… Full text extraction and analysis
- âœ… Semantic search with embeddings
- âœ… Rich metadata (language, author, topics)
- âœ… Agent-ready API and formats
- âœ… Self-learning categorization

## ğŸ—ï¸ Architecture

```
Input Folder
    â†“
[Content Extraction] â†’ Extract text, metadata, structure
    â†“
[Smart Categorization] â†’ ML-based content classification
    â†“
[Semantic Chunking] â†’ Intelligent text splitting with overlap
    â†“
[Embedding Generation] â†’ Vector representations for search
    â†“
[Multi-Format Export]
    â”œâ”€â”€ organized/         # Categorized files
    â”œâ”€â”€ knowledge/
    â”‚   â”œâ”€â”€ canonical/     # Source documents
    â”‚   â”œâ”€â”€ exchange/      # JSONL chunks
    â”‚   â”œâ”€â”€ lakehouse/     # Parquet tables
    â”‚   â”œâ”€â”€ embeddings/    # Vector data
    â”‚   â””â”€â”€ indexes/       # FAISS indexes
    â”œâ”€â”€ metadata.db        # SQLite database
    â””â”€â”€ API Server         # RESTful endpoints
```

## ğŸ¯ Key Features

### 1. **Intelligent Categorization**
- Content-based classification, not just file extensions
- Learns from document structure and keywords
- Adapts to any domain (legal, medical, technical, etc.)

### 2. **RAG-Ready Output**
- **JSONL**: Streamable chunks for LLM pipelines
- **Parquet**: Columnar format for analytics
- **SQLite**: Queryable metadata with full-text search
- **FAISS**: Vector similarity search

### 3. **Semantic Chunking**
- Respects document structure (headings, paragraphs)
- Configurable chunk size (500-1200 tokens)
- Smart overlap for context preservation
- Title paths for hierarchical navigation

### 4. **Multi-Language Support**
- Automatic language detection
- Unicode normalization
- Per-chunk language tagging

### 5. **Agent Integration**
- RESTful API for queries
- Structured responses with metadata
- Security tiers and access control
- Batch processing endpoints

## ğŸ“¦ Installation

```bash
# Core dependencies
pip install sqlite3 pathlib hashlib

# Advanced features (optional but recommended)
pip install pandas pyarrow              # For Parquet export
pip install tiktoken                    # For accurate tokenization
pip install sentence-transformers       # For embeddings
pip install faiss-cpu                  # For vector search
pip install fastapi uvicorn            # For API server

# Or install all at once
pip install -r requirements.txt
```

## ğŸš€ Quick Start

### Basic Usage
```bash
# Organize any folder with RAG indexing
python intelligent_organizer_rag.py /path/to/messy/folder /path/to/output

# With configuration
python intelligent_organizer_rag.py /downloads /organized --config config.yaml

# Copy files instead of moving
python intelligent_organizer_rag.py /documents /indexed --copy

# Search after processing
python intelligent_organizer_rag.py /data /output --search "machine learning"
```

### Python API
```python
from pathlib import Path
from intelligent_organizer_rag import RAGOrganizer

# Initialize organizer
organizer = RAGOrganizer(
    source_path=Path("/your/messy/folder"),
    output_path=Path("/organized/output"),
    config={
        'chunking': {'target_tokens': 800},
        'embeddings': {'model': 'all-MiniLM-L6-v2'}
    }
)

# Process folder
report = organizer.process_folder()

# Search the knowledge base
results = organizer.search("financial reports", k=5)
for result in results:
    print(f"[{result['similarity']:.3f}] {result['source']}")
    print(f"Text: {result['text'][:200]}...")
```

### Agent Query Interface
```python
# Start the API server
python agent_query_interface.py /path/to/knowledge --server --port 8000

# Or use programmatically
from agent_query_interface import QueryEngine

engine = QueryEngine(Path("/organized/knowledge"))
results = engine.search(
    query="quarterly earnings",
    filters={"language": "en", "category": "Documents"},
    top_k=10
)
```

### API Endpoints
```bash
# Search for content
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{"query": "project timeline", "top_k": 5}'

# Get document chunks
curl http://localhost:8000/document/doc:report:20250926

# Get statistics
curl http://localhost:8000/stats
```

## ğŸ”§ Configuration

Edit `rag_organizer_config.yaml`:

```yaml
chunking:
  target_tokens: 800      # Ideal chunk size
  overlap_tokens: 100     # Context overlap

embeddings:
  model: "all-MiniLM-L6-v2"  # Sentence transformer model
  
categories:
  custom_rules:
    "Research":
      subcategories:
        "Papers":
          extensions: [".pdf"]
          content_patterns: ["abstract", "doi:"]
```

## ğŸ“Š Output Structure

```
output/
â”œâ”€â”€ organized/              # Files organized by category
â”‚   â”œâ”€â”€ Documents/
â”‚   â”‚   â”œâ”€â”€ Reports/
â”‚   â”‚   â”œâ”€â”€ Presentations/
â”‚   â”‚   â””â”€â”€ Spreadsheets/
â”‚   â”œâ”€â”€ Code/
â”‚   â”œâ”€â”€ Media/
â”‚   â””â”€â”€ Archives/
â”‚
â”œâ”€â”€ knowledge/              # RAG-ready knowledge base
â”‚   â”œâ”€â”€ canonical/          # Original documents
â”‚   â”œâ”€â”€ exchange/
â”‚   â”‚   â””â”€â”€ chunks.jsonl    # Streaming chunks
â”‚   â”œâ”€â”€ lakehouse/
â”‚   â”‚   â”œâ”€â”€ chunks.parquet  # Analytics format
â”‚   â”‚   â””â”€â”€ embeddings.parquet
â”‚   â”œâ”€â”€ indexes/
â”‚   â”‚   â””â”€â”€ faiss.index     # Vector search index
â”‚   â””â”€â”€ metadata.db         # SQLite database
â”‚
â”œâ”€â”€ README.md              # Auto-generated summary
â””â”€â”€ organization_report.json  # Detailed statistics
```

## ğŸ¨ Use Cases

### 1. **Research Library**
Organize papers, create searchable knowledge base:
```python
config = {
    'categories': {
        'custom_rules': {
            'Research': {
                'subcategories': {
                    'Papers': {'extensions': ['.pdf'], 
                              'content_patterns': ['abstract', 'references']}
                }
            }
        }
    }
}
```

### 2. **Code Repository Analysis**
Index codebases for documentation and search:
```python
config = {
    'extraction': {
        'extract_text': True,
        'max_file_size_mb': 10
    },
    'categories': {
        'custom_rules': {
            'Code': {
                'subcategories': {
                    'Python': {'extensions': ['.py'], 
                              'content_patterns': ['def ', 'class ']}
                }
            }
        }
    }
}
```

### 3. **Company Knowledge Base**
Build searchable corporate memory:
```python
config = {
    'security': {
        'default_tier': 'internal',
        'pii_detection': True
    },
    'rag': {
        'collections': [
            {'name': 'policies', 'security': 'internal'},
            {'name': 'public', 'security': 'public'}
        ]
    }
}
```

## ğŸ” Advanced Queries

### Semantic Search
```python
# Find similar content
results = engine.search(
    query="machine learning deployment strategies",
    top_k=10,
    min_score=0.7
)
```

### Filtered Search
```python
# Search with metadata filters
results = engine.search(
    query="budget",
    filters={
        'category': 'Documents',
        'language': 'en',
        'tags': ['finance', 'quarterly']
    }
)
```

### Multi-Modal Search (Coming Soon)
```python
# Search across text and images
results = engine.multimodal_search(
    text_query="product design",
    image_similarity=uploaded_image,
    top_k=20
)
```

## ğŸ“ˆ Performance

| Metric | Original | New System | Improvement |
|--------|----------|------------|-------------|
| Files/sec | 1000 | 50-100 | Deeper analysis |
| Searchable | No | Yes | âˆ |
| Metadata extracted | No | Yes | âˆ |
| Content understanding | None | Full | âˆ |
| Agent-ready | No | Yes | âˆ |
| Domains supported | 1 | Unlimited | âˆ |

## ğŸ› ï¸ Extending the System

### Add Custom Extractors
```python
class PDFExtractor:
    def extract(self, file_path):
        # Custom PDF extraction logic
        return extracted_text, metadata
```

### Add Custom Categories
```python
categories = {
    'Legal': {
        'subcategories': {
            'Contracts': {
                'extensions': ['.pdf', '.docx'],
                'content_patterns': ['whereas', 'party', 'agreement']
            }
        }
    }
}
```

### Add Custom Embeddings
```python
from transformers import AutoModel

class CustomEmbedder:
    def __init__(self, model_name):
        self.model = AutoModel.from_pretrained(model_name)
    
    def encode(self, texts):
        # Custom encoding logic
        return embeddings
```

## ğŸ”’ Security & Privacy

- **Security Tiers**: Public, Internal, Confidential, Restricted
- **PII Detection**: Optional scanning for personal information
- **Access Control**: API-level filtering based on security clearance
- **Audit Logging**: Track all access and modifications

## ğŸ“ JSONL Schema

Each chunk in the JSONL output follows this schema:

```json
{
  "id": "chunk:doc:report:0001",
  "doc_id": "doc:report:20250926",
  "collection": "default",
  "text": "Actual chunk text...",
  "title_path": ["Report", "Section 1", "Introduction"],
  "source": "/organized/Documents/Reports/annual_report.pdf",
  "chunk_ix": 0,
  "chunk_size_tokens": 800,
  "overlap_tokens": 100,
  "hash": "sha256:...",
  "created_at": "2025-09-26T12:00:00Z",
  "updated_at": "2025-09-26T12:00:00Z",
  "lang": "en",
  "tags": ["finance", "annual", "report"],
  "security": "internal"
}
```

## ğŸ¤ Integration with LLM Frameworks

### LangChain
```python
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# Load the generated index
embeddings = HuggingFaceEmbeddings()
vector_store = FAISS.load_local("/output/knowledge/indexes", embeddings)
```

### LlamaIndex
```python
from llama_index import SimpleDirectoryReader, VectorStoreIndex

# Use the organized documents
documents = SimpleDirectoryReader('/output/organized/Documents').load_data()
index = VectorStoreIndex.from_documents(documents)
```

## ğŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| No embeddings generated | Install `sentence-transformers` |
| FAISS not working | Install `faiss-cpu` or `faiss-gpu` |
| Slow processing | Reduce `chunk_size_tokens` or enable parallel processing |
| Memory issues | Process in batches, reduce `memory_limit_gb` |
| API not starting | Install `fastapi uvicorn` |

## ğŸ“š References

- [Sentence Transformers](https://www.sbert.net/)
- [FAISS](https://github.com/facebookresearch/faiss)
- [Apache Parquet](https://parquet.apache.org/)
- [FastAPI](https://fastapi.tiangolo.com/)

## ğŸ“„ License

MIT License - Use freely for any purpose

## ğŸ™ Credits

Created by Rod Sanchez following The Pragmatic Programmer principles and the RAG Knowledge Format Playbook.

---

**Ready to transform your chaotic folders into intelligent, searchable knowledge bases!** ğŸš€
