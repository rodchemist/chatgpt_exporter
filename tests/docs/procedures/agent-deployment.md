# Agent Deployment Instructions for Documentation System Testing

## üéØ MISSION
Test the Universal Documentation System (UDS) with 9 agent teams across 3 AI models to evaluate implementation consistency and quality.

---

## üìã DEPLOYMENT CONFIGURATION

### Team Assignments:
- **Codex Teams**: 3 teams (codex_team_1, codex_team_2, codex_team_3)
- **Gemini Teams**: 3 teams (gemini_team_1, gemini_team_2, gemini_team_3)
- **Qwen Teams**: 3 teams (qwen_team_1, qwen_team_2, qwen_team_3)

### Test Environment:
- **Base Path**: `/mnt/c/_rod/Documentation_Center/tests/[team_name]/`
- **Source Material**: AI_Chats_Exporter.zip (extracted in base directory)
- **Instructions**: UNIVERSAL_DOCUMENTATION_SYSTEM_README.md
- **Evaluation**: AGENT_EVALUATION_FRAMEWORK.md

---

## üöÄ AGENT DEPLOYMENT PROTOCOL

### For Each Agent Team:

1. **Initialize Team Directory**
   ```bash
   cd tests/[team_name]
   cp ../../AI_Chats_Exporter.zip .
   cp ../../UNIVERSAL_DOCUMENTATION_SYSTEM_README.md .
   ```

2. **Agent Instructions**
   ```
   TASK: Implement Universal Documentation System

   INPUTS:
   - AI_Chats_Exporter.zip (your target codebase to transform)
   - UNIVERSAL_DOCUMENTATION_SYSTEM_README.md (complete instructions)

   REQUIREMENTS:
   - Follow README instructions EXACTLY
   - Create complete self-documenting system
   - Achieve 100% verification score
   - Implement all required features

   SUCCESS CRITERIA:
   - All directories and files created
   - Documentation engine fully functional
   - Quality score ‚â•80%
   - Continuous monitoring active
   ```

3. **Evaluation Process**
   - Run verification commands
   - Score against evaluation framework
   - Document results and issues
   - Compare across teams

---

## üîß DEPLOYMENT SCRIPTS

### Deploy All Teams Script:
```bash
#!/bin/bash
# deploy_all_teams.sh

TEAMS=(
    "codex_team_1" "codex_team_2" "codex_team_3"
    "gemini_team_1" "gemini_team_2" "gemini_team_3"
    "qwen_team_1" "qwen_team_2" "qwen_team_3"
)

for team in "${TEAMS[@]}"; do
    echo "üöÄ Deploying $team..."
    cd tests/$team

    # Copy required files
    cp ../../AI_Chats_Exporter.zip .
    cp ../../UNIVERSAL_DOCUMENTATION_SYSTEM_README.md .

    # Create deployment timestamp
    echo "Deployed: $(date)" > deployment_log.txt

    echo "‚úÖ $team ready for agent deployment"
    cd ../..
done
```

---

## üìä EVALUATION SCRIPT

### Evaluate Team Implementation:
```bash
#!/bin/bash
# evaluate_team.sh [team_name]

TEAM=$1
if [ -z "$TEAM" ]; then
    echo "Usage: ./evaluate_team.sh [team_name]"
    exit 1
fi

cd tests/$TEAM

echo "üîç Evaluating $TEAM implementation..."

# Initialize scores
COMPLETENESS_SCORE=0
QUALITY_SCORE=0
FUNCTIONALITY_SCORE=0
AUTOMATION_SCORE=0

# Test 1: Directory Structure (10 points)
echo "Testing directory structure..."
REQUIRED_DIRS=(
    ".documentation/templates"
    ".documentation/cache"
    ".documentation/logs"
    "docs/api"
    "docs/guides"
    "docs/architecture"
    "docs/procedures"
    "docs/quality"
    "records/audits"
    "records/reviews"
    "records/corrective_actions"
    "records/training"
    "src"
    "tests"
    "agents/config"
    "agents/scripts"
    "agents/outputs"
)

DIR_COUNT=0
for dir in "${REQUIRED_DIRS[@]}"; do
    if [ -d "$dir" ]; then
        ((DIR_COUNT++))
    fi
done

DIR_PERCENTAGE=$((DIR_COUNT * 100 / ${#REQUIRED_DIRS[@]}))
if [ $DIR_PERCENTAGE -ge 100 ]; then
    DIR_SCORE=10
elif [ $DIR_PERCENTAGE -ge 90 ]; then
    DIR_SCORE=8
elif [ $DIR_PERCENTAGE -ge 80 ]; then
    DIR_SCORE=6
elif [ $DIR_PERCENTAGE -ge 70 ]; then
    DIR_SCORE=4
else
    DIR_SCORE=0
fi

# Test 2: Required Files (10 points)
echo "Testing required files..."
REQUIRED_FILES=(
    "README.md"
    "CHANGELOG.md"
    "ARCHITECTURE.md"
    "API_REFERENCE.md"
    "DEPLOYMENT.md"
    "TROUBLESHOOTING.md"
    "CONTRIBUTING.md"
    "SECURITY.md"
    "LICENSE"
)

FILE_COUNT=0
for file in "${REQUIRED_FILES[@]}"; do
    if [ -f "$file" ]; then
        ((FILE_COUNT++))
    fi
done

FILE_PERCENTAGE=$((FILE_COUNT * 100 / ${#REQUIRED_FILES[@]}))
if [ $FILE_PERCENTAGE -ge 100 ]; then
    FILE_SCORE=10
elif [ $FILE_PERCENTAGE -ge 89 ]; then
    FILE_SCORE=8
elif [ $FILE_PERCENTAGE -ge 78 ]; then
    FILE_SCORE=6
elif [ $FILE_PERCENTAGE -ge 56 ]; then
    FILE_SCORE=4
else
    FILE_SCORE=0
fi

# Test 3: Core Engine (20 points)
echo "Testing core engine..."
ENGINE_SCORE=0
if [ -f "agents/scripts/documentation_engine.py" ]; then
    # Check if Python file compiles
    if python3 -m py_compile agents/scripts/documentation_engine.py 2>/dev/null; then
        ENGINE_SCORE=10

        # Check for key functions
        FUNCTIONS=(
            "DocumentationEngine"
            "initialize_system"
            "initialize_database"
            "scan_project"
            "generate_documentation"
            "verify_implementation"
        )

        FUNC_COUNT=0
        for func in "${FUNCTIONS[@]}"; do
            if grep -q "$func" agents/scripts/documentation_engine.py; then
                ((FUNC_COUNT++))
            fi
        done

        FUNC_PERCENTAGE=$((FUNC_COUNT * 100 / ${#FUNCTIONS[@]}))
        if [ $FUNC_PERCENTAGE -ge 100 ]; then
            ENGINE_SCORE=20
        elif [ $FUNC_PERCENTAGE -ge 90 ]; then
            ENGINE_SCORE=16
        elif [ $FUNC_PERCENTAGE -ge 80 ]; then
            ENGINE_SCORE=12
        elif [ $FUNC_PERCENTAGE -ge 70 ]; then
            ENGINE_SCORE=8
        fi
    fi
fi

# Calculate Implementation Completeness
COMPLETENESS_SCORE=$((DIR_SCORE + FILE_SCORE + ENGINE_SCORE))

# Test 4: Functionality
echo "Testing functionality..."
FUNCTIONALITY_SCORE=0

# Test database
if [ -f ".documentation/tracking.db" ]; then
    FUNCTIONALITY_SCORE=$((FUNCTIONALITY_SCORE + 8))
fi

# Test templates
TEMPLATE_COUNT=$(find .documentation/templates -name "*.md" 2>/dev/null | wc -l)
if [ $TEMPLATE_COUNT -ge 5 ]; then
    FUNCTIONALITY_SCORE=$((FUNCTIONALITY_SCORE + 8))
elif [ $TEMPLATE_COUNT -ge 3 ]; then
    FUNCTIONALITY_SCORE=$((FUNCTIONALITY_SCORE + 6))
elif [ $TEMPLATE_COUNT -ge 1 ]; then
    FUNCTIONALITY_SCORE=$((FUNCTIONALITY_SCORE + 4))
fi

# Test verification if engine exists
if [ -f "agents/scripts/documentation_engine.py" ]; then
    if python3 agents/scripts/documentation_engine.py --verify 2>/dev/null | grep -q "100%"; then
        FUNCTIONALITY_SCORE=$((FUNCTIONALITY_SCORE + 9))
    elif python3 agents/scripts/documentation_engine.py --verify 2>/dev/null | grep -q "Score:"; then
        FUNCTIONALITY_SCORE=$((FUNCTIONALITY_SCORE + 5))
    fi
fi

# Calculate Total Score
TOTAL_SCORE=$((COMPLETENESS_SCORE + QUALITY_SCORE + FUNCTIONALITY_SCORE + AUTOMATION_SCORE))

# Determine Grade
if [ $TOTAL_SCORE -ge 95 ]; then
    GRADE="A+"
elif [ $TOTAL_SCORE -ge 90 ]; then
    GRADE="A"
elif [ $TOTAL_SCORE -ge 85 ]; then
    GRADE="B+"
elif [ $TOTAL_SCORE -ge 80 ]; then
    GRADE="B"
elif [ $TOTAL_SCORE -ge 75 ]; then
    GRADE="C+"
elif [ $TOTAL_SCORE -ge 70 ]; then
    GRADE="C"
elif [ $TOTAL_SCORE -ge 60 ]; then
    GRADE="D"
else
    GRADE="F"
fi

# Generate Report
cat > evaluation_report.txt << EOF
==================================================
AGENT EVALUATION REPORT
==================================================
Team: $TEAM
Date: $(date)
Evaluator: Automated System

SCORES:
-------
Implementation Completeness: $COMPLETENESS_SCORE/40
  - Directory Structure: $DIR_SCORE/10 ($DIR_COUNT/${#REQUIRED_DIRS[@]} dirs)
  - Required Files: $FILE_SCORE/10 ($FILE_COUNT/${#REQUIRED_FILES[@]} files)
  - Core Engine: $ENGINE_SCORE/20

Code Quality: $QUALITY_SCORE/25
  - Structure: TBD/10
  - Error Handling: TBD/8
  - Documentation: TBD/7

Functionality: $FUNCTIONALITY_SCORE/25
  - Database: 8/8 ($([ -f ".documentation/tracking.db" ] && echo "‚úÖ" || echo "‚ùå"))
  - Templates: 8/8 ($TEMPLATE_COUNT templates found)
  - Quality Scoring: TBD/9

Automation & Monitoring: $AUTOMATION_SCORE/10
  - Continuous Monitoring: TBD/5
  - Auto-generation: TBD/5

TOTAL SCORE: $TOTAL_SCORE/100
GRADE: $GRADE

ISSUES FOUND:
EOF

# Add specific issues
if [ $DIR_SCORE -lt 10 ]; then
    echo "- Missing directories: $((${#REQUIRED_DIRS[@]} - DIR_COUNT)) out of ${#REQUIRED_DIRS[@]}" >> evaluation_report.txt
fi

if [ $FILE_SCORE -lt 10 ]; then
    echo "- Missing files: $((${#REQUIRED_FILES[@]} - FILE_COUNT)) out of ${#REQUIRED_FILES[@]}" >> evaluation_report.txt
fi

if [ $ENGINE_SCORE -lt 20 ]; then
    echo "- Documentation engine incomplete or missing" >> evaluation_report.txt
fi

echo "" >> evaluation_report.txt
echo "Report generated: $(date)" >> evaluation_report.txt

echo "üìã Evaluation complete. Results saved to evaluation_report.txt"
echo "üèÜ Final Score: $TOTAL_SCORE/100 (Grade: $GRADE)"
```

---

## üéØ AGENT TESTING PROMPTS

### For Codex Teams:
```
You are a senior software engineer tasked with implementing a Universal Documentation System.

TASK: Transform the AI_Chats_Exporter codebase into a self-documenting, self-maintaining system.

INSTRUCTIONS:
1. Read UNIVERSAL_DOCUMENTATION_SYSTEM_README.md completely
2. Extract AI_Chats_Exporter.zip
3. Follow the README instructions EXACTLY
4. Implement ALL required components
5. Achieve 100% verification score

SUCCESS CRITERIA:
- All directories created as specified
- All required files generated
- Documentation engine fully functional
- Quality score ‚â•80%
- Continuous monitoring active

BEGIN IMPLEMENTATION NOW.
```

### For Gemini Teams:
```
You are an AI software engineer specializing in documentation systems.

OBJECTIVE: Implement a comprehensive documentation system following precise specifications.

PROVIDED MATERIALS:
- AI_Chats_Exporter.zip (target codebase)
- UNIVERSAL_DOCUMENTATION_SYSTEM_README.md (complete instructions)

REQUIREMENTS:
- Follow README instructions with 100% accuracy
- Create self-documenting, self-maintaining system
- Implement automated tracking and quality assurance
- Achieve full system verification

DELIVERABLES:
- Complete directory structure
- All required documentation files
- Functional documentation engine
- Quality metrics ‚â•80%

START IMPLEMENTATION IMMEDIATELY.
```

### For Qwen Teams:
```
As an expert developer, implement a Universal Documentation System.

INPUT FILES:
- AI_Chats_Exporter.zip (codebase to transform)
- UNIVERSAL_DOCUMENTATION_SYSTEM_README.md (implementation guide)

MISSION:
Transform the provided codebase into a self-documenting system with:
- Automated documentation generation
- Quality tracking and assurance
- Continuous improvement capabilities
- Real-time monitoring

IMPLEMENTATION STEPS:
1. Study the README completely
2. Extract the ZIP file
3. Follow ALL instructions precisely
4. Test implementation thoroughly
5. Achieve 100% verification

EXECUTE NOW.
```

---

## üìà COMPARATIVE ANALYSIS

### Metrics to Compare:
1. **Implementation Speed**: Time to complete setup
2. **Accuracy**: Percentage of requirements met correctly
3. **Code Quality**: Structure, error handling, documentation
4. **Innovation**: Additional features or improvements
5. **Robustness**: Error handling and edge cases

### Analysis Framework:
```python
def compare_agent_performance(results):
    """Compare performance across all agent teams"""

    metrics = {
        'codex': {'avg_score': 0, 'teams': []},
        'gemini': {'avg_score': 0, 'teams': []},
        'qwen': {'avg_score': 0, 'teams': []}
    }

    # Collect scores for each agent type
    for team_result in results:
        agent_type = team_result['team'].split('_')[0]
        metrics[agent_type]['teams'].append(team_result['score'])

    # Calculate averages
    for agent_type in metrics:
        teams = metrics[agent_type]['teams']
        if teams:
            metrics[agent_type]['avg_score'] = sum(teams) / len(teams)

    return metrics
```

---

## üîÑ ITERATION PROTOCOL

### For Failed Implementations (<60 points):
1. Analyze failure modes
2. Create enhanced instructions
3. Re-deploy with improvements
4. Compare improvement delta

### For Partial Success (60-84 points):
1. Identify specific gaps
2. Provide targeted guidance
3. Test incremental improvements
4. Measure progression

### For Success (85+ points):
1. Document best practices
2. Extract success patterns
3. Apply to struggling teams
4. Optimize further

---

*Agent Deployment Instructions v1.0.0*
*Universal Documentation System Testing Protocol*